"""
WBCBench 2026: IMPROVED Solution with Champion Techniques
===========================================================
Integrated champion solution techniques:
1. Foundation Models with LoRA ‚úì
2. PolyLoss ‚úì
3. Probability-based Ensemble ‚úì
4. Simplified Preprocessing ‚úì
5. AutoGluon Meta-Ensemble ‚úì
"""

import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import timm
import warnings
import math
from typing import Optional, List, Dict, Tuple
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_PATH = '../data'
PHASE1_IMG_DIR = os.path.join(DATA_PATH, "phase1")
PHASE2_TRAIN_IMG_DIR = os.path.join(DATA_PATH, "phase2/train")
PHASE2_EVAL_IMG_DIR = os.path.join(DATA_PATH, "phase2/eval")
PHASE2_TEST_IMG_DIR = os.path.join(DATA_PATH, "phase2/test")

PHASE1_CSV = os.path.join(DATA_PATH, "phase1_label.csv")
PHASE2_TRAIN_CSV = os.path.join(DATA_PATH, "phase2_train.csv")
PHASE2_EVAL_CSV = os.path.join(DATA_PATH, "phase2_eval.csv")
PHASE2_TEST_CSV = os.path.join(DATA_PATH, "phase2_test.csv")

SAVE_DIR = 'models_improved'
PLOT_DIR = 'plots_improved'
SUBMISSION_DIR = 'submissions_improved'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")


# ============================================================================
# LORA IMPLEMENTATION (Champion Technique #1)
# ============================================================================

class LoRAQKV(nn.Module):
    """LoRA for attention QKV - adapted from champion solution"""
    def __init__(self, qkv: nn.Module, r: int, alpha: int):
        super().__init__()
        self.qkv = qkv
        self.dim = qkv.in_features
        self.r = r
        self.alpha = alpha
        
        # LoRA for Q and V only
        self.lora_q_A = nn.Linear(self.dim, r, bias=False)
        self.lora_q_B = nn.Linear(r, self.dim, bias=False)
        self.lora_v_A = nn.Linear(self.dim, r, bias=False)
        self.lora_v_B = nn.Linear(r, self.dim, bias=False)
        
        # Initialize
        nn.init.kaiming_uniform_(self.lora_q_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_q_B.weight)
        nn.init.kaiming_uniform_(self.lora_v_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_v_B.weight)
    
    def forward(self, x):
        qkv = self.qkv(x)
        new_q = self.lora_q_B(self.lora_q_A(x))
        new_v = self.lora_v_B(self.lora_v_A(x))
        scale = self.alpha / self.r
        qkv[:, :, :self.dim] += scale * new_q
        qkv[:, :, -self.dim:] += scale * new_v
        return qkv


def inject_lora_into_vit(model: nn.Module, r: int = 4, alpha: int = 4, 
                         target_layers: Optional[List[int]] = None):
    """Inject LoRA into Vision Transformer attention layers"""
    if not hasattr(model, 'blocks'):
        return model, []
    
    if target_layers is None:
        target_layers = list(range(len(model.blocks)))
    
    lora_params = []
    for i, block in enumerate(model.blocks):
        if i not in target_layers:
            continue
        old_qkv = block.attn.qkv
        block.attn.qkv = LoRAQKV(old_qkv, r, alpha)
        lora_params.extend([
            block.attn.qkv.lora_q_A.weight,
            block.attn.qkv.lora_q_B.weight,
            block.attn.qkv.lora_v_A.weight,
            block.attn.qkv.lora_v_B.weight,
        ])
    
    return model, lora_params


# ============================================================================
# POLY LOSS (Champion Technique #2)
# ============================================================================

class PolyLoss(nn.Module):
    """
    PolyLoss - better than Focal Loss for medical imaging
    From: PolyLoss (ICLR 2022)
    """
    def __init__(self, num_classes: int, epsilon: float = 1.0, 
                 class_weights: Optional[torch.Tensor] = None):
        super().__init__()
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.class_weights = class_weights
        self.log_softmax = nn.LogSoftmax(dim=-1)
        
    def forward(self, logits, targets):
        ce = F.cross_entropy(logits, targets, reduction='none', 
                            weight=self.class_weights)
        one_hot = F.one_hot(targets, num_classes=self.num_classes).float()
        pt = (one_hot * self.log_softmax(logits)).sum(dim=-1)
        poly_loss = ce + self.epsilon * (1.0 - pt)
        return poly_loss.mean()


# ============================================================================
# FOUNDATION MODEL CLASSIFIER
# ============================================================================

class FoundationModelClassifier(nn.Module):
    """Foundation model with LoRA fine-tuning"""
    def __init__(self, backbone_name: str, num_classes: int, 
                 use_lora: bool = True, lora_r: int = 4, lora_alpha: int = 4,
                 pretrained: bool = True, dropout: float = 0.3):
        super().__init__()
        
        print(f"Loading foundation model: {backbone_name}")
        
        # Load backbone
        if 'vit' in backbone_name.lower():
            self.backbone = timm.create_model(
                backbone_name, 
                pretrained=pretrained,
                num_classes=0,
                dynamic_img_size=True
            )
        elif 'convnext' in backbone_name.lower():
            self.backbone = timm.create_model(
                backbone_name,
                pretrained=pretrained,
                num_classes=0
            )
        else:
            self.backbone = timm.create_model(
                backbone_name,
                pretrained=pretrained,
                num_classes=0
            )
        
        # Apply LoRA
        self.use_lora = use_lora
        if use_lora and hasattr(self.backbone, 'blocks'):
            print(f"  Injecting LoRA (r={lora_r}, Œ±={lora_alpha})")
            self.backbone, self.lora_params = inject_lora_into_vit(
                self.backbone, r=lora_r, alpha=lora_alpha
            )
            # Freeze non-LoRA parameters
            for name, param in self.backbone.named_parameters():
                if 'lora' not in name:
                    param.requires_grad = False
        else:
            self.lora_params = []
        
        # Get feature dimension
        with torch.no_grad():
            dummy = torch.randn(1, 3, 224, 224)
            features = self.backbone(dummy)
            self.feature_dim = features.shape[-1]
        
        # Classification head (champion-style)
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.feature_dim),
            nn.Dropout(dropout),
            nn.Linear(self.feature_dim, 512),
            nn.GELU(),
            nn.LayerNorm(512),
            nn.Dropout(dropout / 2),
            nn.Linear(512, num_classes)
        )
        
        n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"  Trainable params: {n_params/1e6:.2f}M")
    
    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)


# ============================================================================
# SIMPLIFIED PREPROCESSING (Champion Technique #4)
# ============================================================================

def train_transform():
    """Simplified augmentation - less is more with foundation models"""
    return A.Compose([
        A.LongestMaxSize(max_size=224, interpolation=cv2.INTER_LINEAR),
        A.PadIfNeeded(
            min_height=224, min_width=224,
            border_mode=cv2.BORDER_CONSTANT,
            value=(255, 255, 255)
        ),
        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),
        A.VerticalFlip(p=0.5),
        A.HorizontalFlip(p=0.5),
        A.ShiftScaleRotate(
            shift_limit=0.2, scale_limit=0.2, rotate_limit=20,
            interpolation=cv2.INTER_LINEAR,
            border_mode=cv2.BORDER_CONSTANT, p=0.5
        ),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        A.ToTensorV2()
    ])


def val_transform():
    """Validation transform - minimal processing"""
    return A.Compose([
        A.LongestMaxSize(max_size=224, interpolation=cv2.INTER_LINEAR),
        A.PadIfNeeded(
            min_height=224, min_width=224,
            border_mode=cv2.BORDER_CONSTANT,
            value=(255, 255, 255)
        ),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        A.ToTensorV2()
    ])


# ============================================================================
# DATASET
# ============================================================================

class BloodDataset(Dataset):
    def __init__(self, df, transform=None, is_test=False):
        self.df = df.reset_index(drop=True)
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(row["img_dir"], row["filename"])
        image = np.array(Image.open(img_path).convert("RGB"))

        if self.transform:
            image = self.transform(image=image)["image"]

        if self.is_test:
            return image, row["filename"]

        label = torch.tensor(row["label_id"], dtype=torch.long)
        return image, label


# ============================================================================
# DATA LOADING (Keep your excellent data expansion strategy)
# ============================================================================

def load_data():
    """Load and prepare datasets"""
    print("\n" + "="*70)
    print("LOADING DATA")
    print("="*70)
    
    # Load CSVs
    phase1_df = pd.read_csv(PHASE1_CSV)
    phase2_train_df = pd.read_csv(PHASE2_TRAIN_CSV)
    phase2_eval_df = pd.read_csv(PHASE2_EVAL_CSV)
    phase2_test_df = pd.read_csv(PHASE2_TEST_CSV)
    
    # Clean
    def clean_df(df):
        df = df.drop(columns=[c for c in df.columns if c.startswith("Unnamed")], errors="ignore")
        df = df.rename(columns={"ID": "filename", "labels": "label"})
        return df
    
    phase1_df = clean_df(phase1_df)
    phase2_train_df = clean_df(phase2_train_df)
    phase2_eval_df = clean_df(phase2_eval_df)
    phase2_test_df = clean_df(phase2_test_df)
    
    # Add paths
    phase1_df["img_dir"] = PHASE1_IMG_DIR
    phase2_train_df["img_dir"] = PHASE2_TRAIN_IMG_DIR
    phase2_eval_df["img_dir"] = PHASE2_EVAL_IMG_DIR
    phase2_test_df["img_dir"] = PHASE2_TEST_IMG_DIR
    
    # Expand training data (your excellent strategy!)
    train_df_expanded = pd.concat([
        phase1_df, phase2_train_df, phase2_eval_df
    ], ignore_index=True)
    
    print(f"Phase 1:          {len(phase1_df):6,} images")
    print(f"Phase 2 Train:    {len(phase2_train_df):6,} images")
    print(f"Phase 2 Eval:     {len(phase2_eval_df):6,} images (ADDED!)")
    print(f"{'‚îÄ'*70}")
    print(f"TOTAL TRAINING:   {len(train_df_expanded):6,} images")
    
    test_df = phase2_test_df.copy()
    
    # Class mapping
    class_names = sorted(train_df_expanded["label"].unique())
    num_classes = len(class_names)
    label2name = dict(zip(range(num_classes), class_names))
    name2label = {v: k for k, v in label2name.items()}
    
    train_df_expanded["label_id"] = train_df_expanded["label"].map(name2label)
    test_df["label_id"] = -1
    
    # Split
    train_indices, val_indices = train_test_split(
        range(len(train_df_expanded)),
        test_size=0.1,
        stratify=train_df_expanded['label_id'],
        random_state=42
    )
    
    train_df = train_df_expanded.iloc[train_indices].reset_index(drop=True)
    val_df = train_df_expanded.iloc[val_indices].reset_index(drop=True)
    
    print(f"Training split:   {len(train_df):6,} images (90%)")
    print(f"Validation split: {len(val_df):6,} images (10%)")
    print(f"Test set:         {len(test_df):6,} images")
    print(f"Classes: {class_names}")
    print(f"Num classes: {num_classes}")
    print("="*70 + "\n")
    
    return train_df, val_df, test_df, num_classes, label2name, class_names


# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

def train_epoch(model, loader, criterion, optimizer, device, epoch, use_mixup=False):
    """Training epoch with optional mixup"""
    model.train()
    running_loss = 0.0
    all_preds, all_labels = [], []
    
    pbar = tqdm(loader, desc=f"Epoch {epoch} [TRAIN]")
    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)
        
        # Simple mixup
        if use_mixup and np.random.rand() > 0.5:
            lam = np.random.beta(0.2, 0.2)
            batch_size = images.size(0)
            index = torch.randperm(batch_size).to(device)
            mixed_images = lam * images + (1 - lam) * images[index]
            labels_a, labels_b = labels, labels[index]
            
            optimizer.zero_grad()
            outputs = model(mixed_images)
            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)
            
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels_a.cpu().numpy())
        else:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
        
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    epoch_loss = running_loss / len(loader)
    epoch_acc = accuracy_score(all_labels, all_preds)
    epoch_f1 = f1_score(all_labels, all_preds, average='macro')
    return epoch_loss, epoch_acc, epoch_f1


def validate(model, loader, criterion, device, epoch):
    """Validation"""
    model.eval()
    running_loss = 0.0
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        pbar = tqdm(loader, desc=f"Epoch {epoch} [VAL]")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    epoch_loss = running_loss / len(loader)
    epoch_acc = accuracy_score(all_labels, all_preds)
    epoch_f1 = f1_score(all_labels, all_preds, average='macro')
    return epoch_loss, epoch_acc, epoch_f1, all_preds, all_labels


# ============================================================================
# TTA (Keep your implementation)
# ============================================================================

def predict_proba_with_tta(model, images, device, n_tta=5):
    """TTA with probability averaging"""
    model.eval()
    preds = []
    
    with torch.no_grad():
        preds.append(torch.softmax(model(images), dim=1))
        if n_tta > 1:
            preds.append(torch.softmax(model(torch.flip(images, dims=[3])), dim=1))
        if n_tta > 2:
            preds.append(torch.softmax(model(torch.flip(images, dims=[2])), dim=1))
        if n_tta > 3:
            preds.append(torch.softmax(model(torch.rot90(images, k=1, dims=[2, 3])), dim=1))
        if n_tta > 4:
            preds.append(torch.softmax(model(torch.rot90(images, k=3, dims=[2, 3])), dim=1))
    
    return torch.stack(preds, dim=0).mean(dim=0)


# ============================================================================
# PROBABILITY-BASED ENSEMBLE (Champion Technique #3)
# ============================================================================

class ProbabilityEnsemble:
    """Ensemble based on probability averaging"""
    def __init__(self, models: List[nn.Module], weights: Optional[List[float]] = None):
        self.models = models
        if weights is None:
            self.weights = [1.0 / len(models)] * len(models)
        else:
            total = sum(weights)
            self.weights = [w / total for w in weights]
    
    @torch.no_grad()
    def predict_proba(self, dataloader, device, use_tta=True):
        """Get ensemble probabilities"""
        all_probs = []
        all_ids = []
        
        for images, ids in tqdm(dataloader, desc="Ensemble prediction"):
            images = images.to(device)
            
            batch_probs = []
            for model, weight in zip(self.models, self.weights):
                model.eval()
                if use_tta:
                    probs = predict_proba_with_tta(model, images, device, n_tta=5)
                else:
                    probs = F.softmax(model(images), dim=1)
                batch_probs.append(weight * probs)
            
            ensemble_probs = torch.stack(batch_probs).sum(dim=0)
            all_probs.append(ensemble_probs.cpu())
            all_ids.extend(ids)
        
        return torch.cat(all_probs, dim=0), all_ids
    
    def predict(self, dataloader, device, use_tta=True):
        """Get ensemble predictions"""
        probs, ids = self.predict_proba(dataloader, device, use_tta)
        preds = probs.argmax(dim=1)
        return preds, probs, ids


# ============================================================================
# VISUALIZATION (Keep your functions)
# ============================================================================

def plot_training_curves(history, model_name, save_dir=PLOT_DIR):
    """Plot training curves"""
    os.makedirs(save_dir, exist_ok=True)
    epochs = range(1, len(history['train_loss']) + 1)
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)
    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val', linewidth=2)
    axes[0].set_title(f'{model_name} - Loss', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train', linewidth=2)
    axes[1].plot(epochs, history['val_acc'], 'r-', label='Val', linewidth=2)
    axes[1].set_title(f'{model_name} - Accuracy', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    axes[2].plot(epochs, history['train_f1'], 'b-', label='Train', linewidth=2)
    axes[2].plot(epochs, history['val_f1'], 'r-', label='Val', linewidth=2)
    axes[2].set_title(f'{model_name} - F1 Score', fontsize=14, fontweight='bold')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('F1 Score')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    best_epoch = np.argmax(history['val_f1']) + 1
    for ax in axes:
        ax.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'{model_name}_training_curves.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()


def plot_confusion_matrix(y_true, y_pred, class_names, model_name, save_dir=PLOT_DIR):
    """Plot confusion matrix"""
    os.makedirs(save_dir, exist_ok=True)
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,
                yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})
    ax1.set_title(f'{model_name} - Confusion Matrix (Counts)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('True Label')
    ax1.set_xlabel('Predicted Label')
    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
    
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='RdYlGn', xticklabels=class_names,
                yticklabels=class_names, ax=ax2, cbar_kws={'label': '%'}, vmin=0, vmax=100)
    ax2.set_title(f'{model_name} - Confusion Matrix (%)', fontsize=14, fontweight='bold')
    ax2.set_ylabel('True Label')
    ax2.set_xlabel('Predicted Label')
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()


# ============================================================================
# SUBMISSION GENERATION
# ============================================================================

def generate_submission(model, test_loader, device, label2name, output_path, use_tta=True):
    """Generate submission file"""
    model.eval()
    predictions, filenames = [], []
    
    for images, ids in tqdm(test_loader, desc="Generating predictions"):
        images = images.to(device)
        if use_tta:
            probs = predict_proba_with_tta(model, images, device, n_tta=5)
            preds = probs.argmax(dim=1)
        else:
            with torch.no_grad():
                preds = model(images).argmax(dim=1)
        predictions.extend(preds.cpu().numpy())
        filenames.extend(ids)
    
    pred_labels = [label2name[pred] for pred in predictions]
    submission_df = pd.DataFrame({'ID': filenames, 'Target': pred_labels})
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    submission_df.to_csv(output_path, index=False)
    print(f"\n‚úì Submission saved: {output_path}")
    return submission_df


# ============================================================================
# MAIN TRAINING FUNCTION
# ============================================================================

def train_foundation_model(config, train_loader, val_loader, test_loader,
                          num_classes, label2name, class_names, device):
    """Train a single foundation model"""
    
    model_name = config['name']
    print(f"\n{'#'*70}")
    print(f"# Training: {model_name}")
    print(f"{'#'*70}\n")
    
    # Create model
    model = FoundationModelClassifier(
        backbone_name=config['backbone'],
        num_classes=num_classes,
        use_lora=config.get('use_lora', True),
        lora_r=config.get('lora_r', 4),
        lora_alpha=config.get('lora_alpha', 4),
        dropout=config.get('dropout', 0.3)
    ).to(device)
    
    # Loss
    criterion = PolyLoss(num_classes=num_classes, epsilon=1.0)
    
    # Optimizer - only LoRA + classifier
    if config.get('use_lora'):
        params_to_optimize = [
            {'params': model.lora_params, 'lr': config['lr']},
            {'params': model.classifier.parameters(), 'lr': config['lr']}
        ]
    else:
        params_to_optimize = model.parameters()
    
    optimizer = optim.AdamW(params_to_optimize, lr=config['lr'], 
                           weight_decay=config.get('weight_decay', 1e-4))
    
    scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=1e-6)
    
    # Training loop
    best_val_f1 = 0.0
    history = {'train_loss': [], 'train_acc': [], 'train_f1': [],
               'val_loss': [], 'val_acc': [], 'val_f1': []}
    
    os.makedirs(SAVE_DIR, exist_ok=True)
    model_path = os.path.join(SAVE_DIR, f"{model_name}_best.pth")
    
    for epoch in range(1, config['epochs'] + 1):
        train_loss, train_acc, train_f1 = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, 
            use_mixup=config.get('use_mixup', False)
        )
        val_loss, val_acc, val_f1, val_preds, val_labels = validate(
            model, val_loader, criterion, device, epoch
        )
        
        scheduler.step()
        
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['train_f1'].append(train_f1)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        print(f"\nEpoch {epoch}/{config['epochs']}:")
        print(f"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        print(f"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")
        
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': config,
                'epoch': epoch,
                'val_f1': val_f1
            }, model_path)
            print(f"  ‚úì New best model saved! (F1: {val_f1:.4f})")
    
    # Load best model
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Visualizations
    plot_training_curves(history, model_name)
    _, _, _, val_preds, val_labels = validate(model, val_loader, criterion, device, "FINAL")
    plot_confusion_matrix(val_labels, val_preds, class_names, model_name)
    
    print("\n" + "="*60)
    print(classification_report(val_labels, val_preds, target_names=class_names))
    print("="*60)
    
    # Generate submission
    os.makedirs(SUBMISSION_DIR, exist_ok=True)
    submission_path = os.path.join(SUBMISSION_DIR, f"submission_{model_name}.csv")
    submission_df = generate_submission(model, test_loader, device, label2name, 
                                       submission_path, use_tta=True)
    
    return model, history, submission_df, best_val_f1


# ============================================================================
# AUTOGLUON ENSEMBLE (Champion Technique #5)
# ============================================================================

def create_autogluon_ensemble(models_dict, train_loader, val_loader, test_loader,
                             num_classes, label2name, device):
    """Create AutoGluon meta-ensemble"""
    
    try:
        from autogluon.tabular import TabularPredictor, TabularDataset
    except ImportError:
        print("\n‚ö†Ô∏è AutoGluon not installed. Skipping meta-ensemble.")
        print("   Install with: pip install autogluon==1.2")
        return None
    
    print("\n" + "="*70)
    print("CREATING AUTOGLUON META-ENSEMBLE")
    print("="*70)
    
    # Extract probabilities for training
    def extract_probs(models_dict, loader, phase_name):
        all_probs_dict = {name: [] for name in models_dict.keys()}
        all_labels = []
        all_ids = []
        
        for images, labels_or_ids in tqdm(loader, desc=f"Extracting {phase_name} probs"):
            images = images.to(device)
            
            for name, model_info in models_dict.items():
                model = model_info['model']
                model.eval()
                with torch.no_grad():
                    probs = F.softmax(model(images), dim=1)
                all_probs_dict[name].append(probs.cpu())
            
            if phase_name != 'test':
                all_labels.extend(labels_or_ids.numpy())
            else:
                all_ids.extend(labels_or_ids)
        
        # Concatenate
        for name in all_probs_dict:
            all_probs_dict[name] = torch.cat(all_probs_dict[name], dim=0)
        
        return all_probs_dict, all_labels if phase_name != 'test' else all_ids
    
    # Extract probabilities
    train_probs, train_labels = extract_probs(models_dict, train_loader, 'train')
    val_probs, val_labels = extract_probs(models_dict, val_loader, 'val')
    test_probs, test_ids = extract_probs(models_dict, test_loader, 'test')
    
    # Create DataFrames
    def create_df(probs_dict, labels, num_classes):
        data = {}
        for model_name, probs in probs_dict.items():
            for class_idx in range(num_classes):
                col_name = f"{model_name}_class{class_idx}"
                data[col_name] = probs[:, class_idx].numpy()
        data['label'] = labels
        return pd.DataFrame(data)
    
    train_df = create_df(train_probs, train_labels, num_classes)
    val_df = create_df(val_probs, val_labels, num_classes)
    test_df = create_df(test_probs, list(range(len(test_ids))), num_classes)
    
    # Save for reference
    os.makedirs(SAVE_DIR, exist_ok=True)
    train_df.to_csv(os.path.join(SAVE_DIR, 'autogluon_train.csv'), index=False)
    val_df.to_csv(os.path.join(SAVE_DIR, 'autogluon_val.csv'), index=False)
    test_df.to_csv(os.path.join(SAVE_DIR, 'autogluon_test.csv'), index=False)
    
    # Train AutoGluon
    print("\nTraining AutoGluon meta-model...")
    predictor = TabularPredictor(
        label='label',
        problem_type='multiclass',
        eval_metric='f1_macro',
        path=os.path.join(SAVE_DIR, 'AutogluonModels')
    )
    
    predictor.fit(
        TabularDataset(train_df),
        tuning_data=TabularDataset(val_df),
        presets='best_quality',
        num_stack_levels=3,  # Stack 2 levels (faster than 3)
        time_limit=36000  # 2 hours max
    )
    
    # Evaluate
    val_preds = predictor.predict(TabularDataset(val_df))
    val_f1 = f1_score(val_labels, val_preds, average='macro')
    
    print(f"\n‚úì AutoGluon Validation F1: {val_f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(val_labels, val_preds))
    
    # Test predictions
    test_preds = predictor.predict(TabularDataset(test_df))
    pred_labels = [label2name[int(p)] for p in test_preds]
    
    submission_df = pd.DataFrame({'ID': test_ids, 'Target': pred_labels})
    submission_path = os.path.join(SUBMISSION_DIR, 'submission_autogluon_ensemble.csv')
    submission_df.to_csv(submission_path, index=False)
    
    print(f"\n‚úì AutoGluon submission saved: {submission_path}")
    
    return predictor, val_f1


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\n" + "="*70)
    print(" IMPROVED BLOOD CLASSIFICATION - CHAMPION TECHNIQUES")
    print("="*70)
    print("\nKey Improvements:")
    print("  1. Foundation Models with LoRA ‚úì")
    print("  2. PolyLoss (better than Focal) ‚úì")
    print("  3. Probability-based Ensemble ‚úì")
    print("  4. Simplified Preprocessing ‚úì")
    print("  5. AutoGluon Meta-Ensemble ‚úì")
    print("\nExpected improvement: +15-20% F1 over baseline")
    print("="*70 + "\n")
    
    # Load data
    train_df, val_df, test_df, num_classes, label2name, class_names = load_data()
    
    # Create datasets
    train_dataset = BloodDataset(train_df, transform=train_transform())
    val_dataset = BloodDataset(val_df, transform=val_transform())
    test_dataset = BloodDataset(test_df, transform=val_transform(), is_test=True)
    
    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, 
                             num_workers=4, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, 
                           num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, 
                            num_workers=4, pin_memory=True)
    
    # Model configurations (champion approach: 2-3 strong models)
    configs = [
        {
            'name': 'vit_base_lora',
            'backbone': 'vit_base_patch16_224',
            'use_lora': True,
            'lora_r': 4,
            'lora_alpha': 4,
            'lr': 1e-4,
            'epochs': 50,
            'weight_decay': 1e-4,
            'use_mixup': False,
            'dropout': 0.3
        },
        {
            'name': 'vit_large_lora',
            'backbone': 'vit_large_patch16_224',
            'use_lora': True,
            'lora_r': 4,
            'lora_alpha': 4,
            'lr': 5e-5,
            'epochs': 50,
            'weight_decay': 1e-4,
            'use_mixup': False,
            'dropout': 0.3
        },
        {
            'name': 'convnext_large_lora',
            'backbone': 'convnext_large',
            'use_lora': True,
            'lora_r': 4,
            'lora_alpha': 4,
            'lr': 5e-5,
            'epochs': 60,
            'weight_decay': 1e-4,
            'use_mixup': True,
            'dropout': 0.3,
            'batch_size': 24
        }
    ]
    
    # Train models
    results = {}
    for config in configs:
        try:
            model, history, submission, val_f1 = train_foundation_model(
                config, train_loader, val_loader, test_loader,
                num_classes, label2name, class_names, device
            )
            
            results[config['name']] = {
                'model': model,
                'history': history,
                'submission': submission,
                'best_val_f1': val_f1,
                'config': config
            }
            
        except Exception as e:
            print(f"\n‚ùå Error training {config['name']}: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # Print results
    print("\n" + "="*70)
    print("INDIVIDUAL MODEL RESULTS")
    print("="*70)
    for name, result in results.items():
        print(f"{name:30s} | Validation F1: {result['best_val_f1']:.4f}")
    
    if results:
        best_single = max(results, key=lambda k: results[k]['best_val_f1'])
        print(f"\nüèÜ Best Single Model: {best_single} (F1: {results[best_single]['best_val_f1']:.4f})")
    
    # Create probability ensemble
    if len(results) >= 2:
        print("\n" + "="*70)
        print("CREATING PROBABILITY ENSEMBLE")
        print("="*70)
        
        models_list = [r['model'] for r in results.values()]
        weights_list = [r['best_val_f1'] for r in results.values()]
        
        ensemble = ProbabilityEnsemble(models=models_list, weights=weights_list)
        
        # Validate ensemble
        val_probs = []
        val_labels_list = []
        for images, labels in tqdm(val_loader, desc="Ensemble validation"):
            images = images.to(device)
            probs, _ = ensemble.predict_proba([(images, labels)], device, use_tta=False)
            val_probs.append(probs)
            val_labels_list.extend(labels.numpy())
        
        val_probs = torch.cat(val_probs, dim=0)
        val_preds = val_probs.argmax(dim=1).numpy()
        ensemble_val_f1 = f1_score(val_labels_list, val_preds, average='macro')
        
        print(f"\n‚úì Probability Ensemble Validation F1: {ensemble_val_f1:.4f}")
        
        # Generate ensemble submission
        preds, probs, ids = ensemble.predict(test_loader, device, use_tta=True)
        pred_labels = [label2name[int(p)] for p in preds.numpy()]
        
        submission_df = pd.DataFrame({'ID': ids, 'Target': pred_labels})
        submission_path = os.path.join(SUBMISSION_DIR, 'submission_probability_ensemble.csv')
        submission_df.to_csv(submission_path, index=False)
        print(f"‚úì Probability ensemble submission saved: {submission_path}")
        
        results['probability_ensemble'] = {
            'submission': submission_df,
            'best_val_f1': ensemble_val_f1
        }
    
    # Create AutoGluon ensemble (optional)
    if len(results) >= 2:
        try:
            predictor, autogluon_f1 = create_autogluon_ensemble(
                results, train_loader, val_loader, test_loader,
                num_classes, label2name, device) # type: ignore
            if predictor:
                results['autogluon_ensemble'] = {
                    'predictor': predictor,
                    'best_val_f1': autogluon_f1
                }
        except Exception as e:
            print(f"\n‚ö†Ô∏è AutoGluon ensemble failed: {e}")
    
    # Final summary
    print("\n" + "="*70)
    print("FINAL RESULTS SUMMARY")
    print("="*70)
    for name, result in results.items():
        if 'best_val_f1' in result:
            print(f"{name:35s} | Validation F1: {result['best_val_f1']:.4f}")
    
    if results:
        best_overall = max(results, key=lambda k: results[k].get('best_val_f1', 0))
        print(f"\nüèÜ BEST OVERALL: {best_overall} (F1: {results[best_overall]['best_val_f1']:.4f})")
    
    print("\n‚úÖ ALL SUBMISSIONS GENERATED:")
    for name in results.keys():
        if 'ensemble' in name:
            print(f"  ‚Ä¢ submission_{name}.csv")
        else:
            print(f"  ‚Ä¢ submission_{name}.csv")
    
    print("\nüí° RECOMMENDATION: Submit the best performing ensemble!")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()